{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915b9993-484c-4c88-b9d0-5ad648e25e98",
   "metadata": {},
   "source": [
    "### [A01] Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f930e3-bf8b-497b-8d77-c5b85333619f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "### Import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.window as sw\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f307c-ad6c-4531-b2d6-719165c169be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"cse6242_restaurant_survival\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd182d-24ce-49d3-9c9f-1bc5adb68084",
   "metadata": {},
   "source": [
    "### [B01] Functions\n",
    "This code requires the following datasets to be downloaded and stored locally:<br>\n",
    "- https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/about_data<br>\n",
    "- Place the downloaded data in ./data/raw <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108773cc-0912-4e58-912f-23ff19a1e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0fa0a-7c37-467b-b1f7-b17a7d18e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_raw(\n",
    "        spark:pyspark.sql.SparkSession,\n",
    "        options:dict={}\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Read the necessary files.\n",
    "    \"\"\"\n",
    "    #\n",
    "    ### Read NYC Restaurant Data: DOHMH New York City Restaurant Inspection Results\n",
    "    pdf01_restaurant = pd.read_csv(\"./../data/raw/DOHMH_New_York_City_Restaurant_Inspection_Results.csv\")\n",
    "    columns_pdf01 = pdf01_restaurant.columns\n",
    "    columns_pdf01 = [str(i).lower().replace(\" \",\"_\") for i in columns_pdf01]\n",
    "    pdf01_restaurant.columns = columns_pdf01\n",
    "    pdf01_restaurant[\"inspection_date\"] = pd.to_datetime(pdf01_restaurant[\"inspection_date\"])\n",
    "    df01_nyc_restaurant_inspection_manhattan = (spark.createDataFrame(pdf01_restaurant)\n",
    "        .where(\"(LOWER(boro) = 'manhattan')\")\n",
    "        .selectExpr(\"bin\",\"camis\",\"boro\",\"zipcode\",\"date(inspection_date) as inspection_date_Ymd\",\n",
    "            \"YEAR(date(inspection_date)) as inspection_year\",\"inspection_type\",\"latitude\",\"longitude\")\n",
    "        .distinct())\n",
    "    del pdf01_restaurant\n",
    "    #\n",
    "    df02_buildings = (spark.read.option(\"header\",True).csv(\"./../data/processed/building/building_240405_1230.csv\")\n",
    "        .withColumnRenamed(r\"distance_from_station(ft)\",\"distance_from_station_ft\"))\n",
    "    df03_bus_stop_mn = (spark.read.option(\"header\", True).csv(\"./../data/raw/bus_stop/Bus_Stop_Shelter.csv\"\")\n",
    "        .where(\"(LOWER(BoroName) = 'manhattan')\")\n",
    "        .selectExpr(\"Shelter_ID as shelter_id\",\"BoroName as boro\",\"Latitude as latitude\",\"Longitude as longitude\"))\n",
    "    #\n",
    "    pdf04_mta = (pd.read_csv(\"./../data/raw/mta_station/MTA_Subway_Stations.csv\")\n",
    "        .loc[:, [\"GTFS Stop ID\",\"GTFS Latitude\",\"GTFS Longitude\"]]\n",
    "        .drop_duplicates())\n",
    "    columns_pdf04 = pdf04_mta.columns\n",
    "    columns_pdf04 = [str(i).lower().replace(\" \",\"_\") for i in columns_pdf04]\n",
    "    pdf04_mta.columns = columns_pdf04\n",
    "    df04_mta = spark.createDataFrame(pdf04_mta)\n",
    "    del pdf04_mta\n",
    "    #\n",
    "    df05_ridership = (spark.read.option(\"header\",True).csv(\"./../data/processed/traffic/ridership_by_bin.csv\")\n",
    "        .withColumnRenamed(\"distance_from_station(ft)\",\"dist_station\"))\n",
    "    df06_aadt = spark.read.option(\"header\",True).csv(\"./../data/processed/traffic/idw_aadt_by_bin.csv\")\n",
    "    df07_atvc = spark.read.option(\"header\",True).csv(\"./../data/processed/traffic/idw_atvc_by_bin.csv\")\n",
    "    #\n",
    "    return (df01_nyc_restaurant_inspection_manhattan, df02_buildings, df03_bus_stop_mn, df04_mta,\n",
    "        df05_ridership, df06_aadt, df07_atvc)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3dbd4d-be79-444c-bc02-67f30b56c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bankrupt_restaurants(\n",
    "        df01_nyc_restaurant_inspection_manhattan:pyspark.sql.DataFrame\n",
    "        ) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Restaurants have to be checked annually, and the dataset only shows yearly inspeaction results for three\n",
    "        consecutive years. Thus, take all data where the restaurant's maximum age is 2 years, and check\n",
    "        if the restaurant closed after a year.\n",
    "    \"\"\"\n",
    "    #\n",
    "    case_flag_restaurant_one_year = \"\"\"\n",
    "        CASE\n",
    "            WHEN years_open <= 1 THEN 0\n",
    "            ELSE 1\n",
    "        END AS flag_restaurant_one_year\n",
    "        \"\"\"\n",
    "    #\n",
    "    cond_array = []\n",
    "    cond_array.append(\"(latitude IS NOT NULL)\")\n",
    "    cond_array.append(\"(longitude IS NOT NULL)\")\n",
    "    cond_array.append(\"(latitude != 0.0)\")\n",
    "    cond_array.append(\"(longitude != 0.0)\")\n",
    "    filter_cond = \" AND \".join(cond_array)\n",
    "    df08_bankrupt_restaurants = (df01_nyc_restaurant_inspection_manhattan\n",
    "        .where(filter_cond)\n",
    "        .groupBy(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .agg(sf.min(\"inspection_year\").alias(\"inspection_year_min\"),\n",
    "            sf.max(\"inspection_year\").alias(\"inspection_year_max\"))\n",
    "        .selectExpr(\"*\",\"(inspection_year_max - inspection_year_min) as years_open\")\n",
    "        .selectExpr(\"*\",case_flag_restaurant_one_year)\n",
    "        .where(\"(inspection_year_min >= 2015) AND (years_open <= 2)\")\n",
    "        .distinct())\n",
    "    #\n",
    "    return df08_bankrupt_restaurants\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b055b1-82b5-4a5d-b415-50bd60256282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buildings_info(\n",
    "        df02_buildings:pyspark.sql.DataFrame,\n",
    "        df05_ridership:pyspark.sql.DataFrame,\n",
    "        df06_aadt:pyspark.sql.DataFrame,\n",
    "        df07_atvc:pyspark.sql.DataFrame\n",
    "        ) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    This functions relies on data that are already stored locally.\n",
    "    \"\"\"\n",
    "    #\n",
    "    tdf01_buildings = (df02_buildings\n",
    "        .selectExpr(\"bin\",\"office_area\",\"retail_area\",\"residential_area\",\"street_width_min\",\"street_width_max\",\n",
    "            \"posted_speed\",\"betweeness\",\"distance_from_station_ft as dist_station\",\"office_within_450ft as office_450\",\n",
    "            \"retail_within_450ft as retail_450\",\"residential_within_450ft as residential_450\",\n",
    "            \"distance_to_park dist_park\",\"distance_to_school as dist_school\"))\n",
    "    #\n",
    "    tdf02_ridership = (df05_ridership\n",
    "        .groupBy(\"bin\")\n",
    "        .agg(sf.mean(\"ridership_morning\").alias(\"ridership_morning_mean\"),\n",
    "            sf.mean(\"ridership_midday\").alias(\"ridership_midday_mean\"),\n",
    "            sf.mean(\"ridership_evening\").alias(\"ridership_evening_mean\"),\n",
    "            sf.mean(\"ridership_night\").alias(\"ridership_night_mean\"),\n",
    "            sf.mean(\"ridership_late_night\").alias(\"ridership_late_night_mean\")))\n",
    "    tdf03_aadt = (df06_aadt\n",
    "        .groupBy(\"bin\")\n",
    "        .agg(sf.mean(\"idw_aadt\").alias(\"idw_aadt_mean\")))\n",
    "    tdf04_atvc = (df07_atvc\n",
    "        .groupBy(\"bin\")\n",
    "        .agg(sf.mean(\"idw_atvc\").alias(\"idw_atvc_mean\")))\n",
    "    #\n",
    "    df09_buildings_info = (tdf01_buildings\n",
    "        .join(tdf02_ridership, how=\"inner\", on=[\"bin\"])\n",
    "        .join(tdf03_aadt, how=\"inner\", on=[\"bin\"])\n",
    "        .join(tdf04_atvc, how=\"inner\", on=[\"bin\"])\n",
    "        .distinct())\n",
    "    #\n",
    "    return df09_buildings_info\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abb221-0387-486c-83b6-848701d8394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proximity_restaurant(\n",
    "        df01_nyc_restaurant_inspection_manhattan:pyspark.sql.DataFrame,\n",
    "        df08_bankrupt_restaurants:pyspark.sql.DataFrame\n",
    "        ) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the number of restaurants within the set distances.\n",
    "    \"\"\"\n",
    "    #\n",
    "    tdf01_unique_lat_lng = (df01_nyc_restaurant_inspection_manhattan\n",
    "        .selectExpr(\"camis as camis_diff\",\"latitude as lat_diff\",\"longitude as lng_diff\")\n",
    "        .distinct())\n",
    "    #\n",
    "    tdf02_open_restaurants = (df08_bankrupt_restaurants\n",
    "        .selectExpr(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .distinct())\n",
    "    #\n",
    "    df10_proximity_restaurant = (tdf02_open_restaurants\n",
    "        .join(tdf01_unique_lat_lng, how=\"cross\")\n",
    "        .where(\"(camis != camis_diff)\")\n",
    "        .withColumn('distance_in_kms' , \\\n",
    "            sf.round((sf.acos((sf.sin(sf.radians(sf.col(\"latitude\"))) * sf.sin(sf.radians(sf.col(\"lat_diff\")))) + \\\n",
    "                ((sf.cos(sf.radians(sf.col(\"latitude\"))) * sf.cos(sf.radians(sf.col(\"lat_diff\")))) * \\\n",
    "                (sf.cos(sf.radians(\"longitude\") - sf.radians(\"lng_diff\"))))) * sf.lit(6371.0)), 4))\n",
    "        .groupBy(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .agg(sf.countDistinct(sf.when((sf.col(\"distance_in_kms\") <= 0.10), \n",
    "                sf.col(\"camis_diff\")).otherwise(None)).alias(\"food_100\"),\n",
    "            sf.countDistinct(sf.when((sf.col(\"distance_in_kms\") <= 0.40), \n",
    "                sf.col(\"camis_diff\")).otherwise(None)).alias(\"food_400\"),\n",
    "            sf.countDistinct(sf.when((sf.col(\"distance_in_kms\") <= 0.80), \n",
    "                sf.col(\"camis_diff\")).otherwise(None)).alias(\"food_800\"),\n",
    "            sf.countDistinct(sf.when((sf.col(\"distance_in_kms\") <= 1.00), \n",
    "                sf.col(\"camis_diff\")).otherwise(None)).alias(\"food_1000\")))\n",
    "    #\n",
    "    return df10_proximity_restaurant\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559492b-6243-45e1-9199-615bc846c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proximity_bus(\n",
    "        df03_bus_stop_mn:pyspark.sql.DataFrame,\n",
    "        df08_bankrupt_restaurants:pyspark.sql.DataFrame\n",
    "        ) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Get proximity of restaurants to bus stops.\n",
    "    \"\"\"\n",
    "    #\n",
    "    tdf01_buses = (df03_bus_stop_mn\n",
    "        .selectExpr(\"shelter_id\",\"latitude as lat_diff\",\"longitude as lng_diff\"))\n",
    "    df11_proximity_bus = (df08_bankrupt_restaurants\n",
    "        .selectExpr(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .join(tdf01_buses, how=\"cross\")\n",
    "        .withColumn(\"distance_in_kms\", \\\n",
    "            sf.round((sf.acos((sf.sin(sf.radians(sf.col(\"latitude\"))) * sf.sin(sf.radians(sf.col(\"lat_diff\")))) + \\\n",
    "                ((sf.cos(sf.radians(sf.col(\"latitude\"))) * sf.cos(sf.radians(sf.col(\"lat_diff\")))) * \\\n",
    "                (sf.cos(sf.radians(\"longitude\") - sf.radians(\"lng_diff\"))))) * sf.lit(6371.0)), 4))\n",
    "        .groupBy(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .agg(sf.countDistinct(sf.when(sf.col(\"distance_in_kms\") <= 0.10,\n",
    "                sf.col(\"shelter_id\")).otherwise(None)).alias(\"bus_100\"),\n",
    "            sf.countDistinct(sf.when(sf.col(\"distance_in_kms\") <= 0.40,\n",
    "                sf.col(\"shelter_id\")).otherwise(None)).alias(\"bus_400\"),\n",
    "            sf.countDistinct(sf.when(sf.col(\"distance_in_kms\") <= 1.00,\n",
    "                sf.col(\"shelter_id\")).otherwise(None)).alias(\"bus_1000\")))\n",
    "    #\n",
    "    return df11_proximity_bus\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81f468-681d-4768-ae04-1d74a1053d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proximity_mta(\n",
    "        df04_mta:pyspark.sql.DataFrame,\n",
    "        df08_bankrupt_restaurants:pyspark.sql.DataFrame\n",
    "        ) -> pyspark.sql.DataFrame:\n",
    "    \"\"\"\n",
    "    Get proximity to MTA.\n",
    "    \"\"\"\n",
    "    #\n",
    "    tdf01_mta = (df04_mta\n",
    "        .selectExpr(\"gtfs_stop_id\",\"gtfs_latitude as lat_diff\",\"gtfs_longitude as lng_diff\"))\n",
    "    df12_proximity_mta = (df08_bankrupt_restaurants\n",
    "        .selectExpr(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .join(tdf01_mta, how=\"cross\")\n",
    "        .withColumn(\"distance_in_kms\", \\\n",
    "            sf.round((sf.acos((sf.sin(sf.radians(sf.col(\"latitude\"))) * sf.sin(sf.radians(sf.col(\"lat_diff\")))) + \\\n",
    "                ((sf.cos(sf.radians(sf.col(\"latitude\"))) * sf.cos(sf.radians(sf.col(\"lat_diff\")))) * \\\n",
    "                (sf.cos(sf.radians(\"longitude\") - sf.radians(\"lng_diff\"))))) * sf.lit(6371.0)), 4))\n",
    "        .groupBy(\"bin\",\"camis\",\"latitude\",\"longitude\")\n",
    "        .agg(sf.countDistinct(sf.when(sf.col(\"distance_in_kms\") <= 0.10,\n",
    "                sf.col(\"gtfs_stop_id\")).otherwise(None)).alias(\"train_100\"),\n",
    "            sf.countDistinct(sf.when(sf.col(\"distance_in_kms\") <= 0.40,\n",
    "                sf.col(\"gtfs_stop_id\")).otherwise(None)).alias(\"train_400\"),\n",
    "            sf.countDistinct(sf.when(sf.col(\"distance_in_kms\") <= 1.00,\n",
    "                sf.col(\"gtfs_stop_id\")).otherwise(None)).alias(\"train_1000\")))\n",
    "    #\n",
    "    return df12_proximity_mta\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fde4f-33a4-4d9f-abdc-f93cd5e79752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(\n",
    "        spark:pyspark.sql.SparkSession,\n",
    "        options:dict={}\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Running this function processes the raw datasets and saves the file as a CSV.\n",
    "    \"\"\"\n",
    "    #\n",
    "    start_time = time.time()\n",
    "    #\n",
    "    (df01_nyc_restaurant_inspection_manhattan, df02_buildings, df03_bus_stop_mn, df04_mta,\n",
    "        df05_ridership, df06_aadt, df07_atvc) = read_data_raw(spark, options)\n",
    "    df08_bankrupt_restaurants = get_bankrupt_restaurants(df01_nyc_restaurant_inspection_manhattan)\n",
    "    df09_buildings_info = get_buildings_info(df02_buildings, df05_ridership, df06_aadt, df07_atvc)\n",
    "    df10_proximity_restaurant = get_proximity_restaurant(df01_nyc_restaurant_inspection_manhattan, df08_bankrupt_restaurants)\n",
    "    df11_proximity_bus = get_proximity_bus(df03_bus_stop_mn, df08_bankrupt_restaurants)\n",
    "    df12_proximity_mta = get_proximity_mta(df04_mta, df08_bankrupt_restaurants)\n",
    "    #\n",
    "    df13_all_features = (df08_bankrupt_restaurants\n",
    "        .join(df09_buildings_info, how=\"left\", on=[\"bin\"])\n",
    "        .join(df10_proximity_restaurant, how=\"left\", on=[\"bin\",\"camis\",\"latitude\",\"longitude\"])\n",
    "        .join(df11_proximity_bus, how=\"left\", on=[\"bin\",\"camis\",\"latitude\",\"longitude\"])\n",
    "        .join(df12_proximity_mta, how=\"left\", on=[\"bin\",\"camis\",\"latitude\",\"longitude\"])\n",
    "        .selectExpr(\"bin\",\"camis\",\n",
    "            \"inspection_year_min as open_year\",\"inspection_year_max as close_year\",\"years_open\",\n",
    "            \"flag_restaurant_one_year\",\"latitude\",\"longitude\",\n",
    "            \"food_100\",\"food_400\",\"food_800\",\"food_1000\",\n",
    "            \"bus_100\",\"bus_400\",\"bus_1000\",\n",
    "            \"train_100\",\"train_400\",\"train_1000\",\n",
    "            \"office_area\",\"retail_area\",\"residential_area\",\"street_width_min\",\"street_width_max\",\"posted_speed\",\n",
    "            \"dist_station\",\"dist_park\",\"dist_school\",\"office_450\",\"retail_450\",\"residential_450\",\"ridership_morning_mean\",\n",
    "            \"ridership_midday_mean\",\"ridership_evening_mean\",\"ridership_night_mean\",\"ridership_late_night_mean\",\n",
    "            \"idw_aadt_mean\",\"idw_atvc_mean\"))\n",
    "    #\n",
    "    cols_analysis = [\"flag_restaurant_one_year\",\"bin\",\"camis\",\"latitude\",\"longitude\",\n",
    "        \"open_year\",\"close_year\",\"years_open\",\n",
    "        \"food_100\",\"food_400\",\"food_800\",\"food_1000\",\n",
    "        \"bus_100\",\"bus_400\",\"bus_1000\",\"train_100\",\"train_400\",\"train_1000\",\n",
    "        \"office_area\",\"retail_area\",\"residential_area\",\"street_width_min\",\"street_width_max\",\"posted_speed\",\n",
    "        \"dist_station\",\"dist_park\",\"dist_school\",\"office_450\",\"retail_450\",\"residential_450\",\n",
    "        \"ridership_morning_mean\",\"ridership_midday_mean\",\"ridership_evening_mean\",\"ridership_night_mean\",\"ridership_late_night_mean\",\n",
    "        \"idw_aadt_mean\",\"idw_atvc_mean\"]\n",
    "    #\n",
    "    pdf13_all_features = (df13_all_features\n",
    "        .toPandas()\n",
    "        .loc[:, cols_analysis]\n",
    "        .dropna()\n",
    "        .reset_index(drop=True))\n",
    "    str_date_today = options.get(\"date_today\", datetime.date.today().strftime(\"%Y%m%d\"))\n",
    "    pdf13_all_features.to_csv(\"./../data/processed/{}_Survival_Features_Labeled.csv\".format(str_date_today))\n",
    "    #\n",
    "    print(\"get_all_features | Done in {:.2f} s\".format(time.time()-start_time))\n",
    "    #\n",
    "    return pdf13_all_features\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2b202-4dda-4c98-bfd7-5f13687f01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_processed(\n",
    "        options:dict={}\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Read the training file and the test file.\n",
    "    \"\"\"\n",
    "    #\n",
    "    str_date_today = options.get(\"date_today\", datetime.date.today().strftime(\"%Y%m%d\"))\n",
    "    pdf14_labeled = pd.read_csv(\"./../data/processed/{}_Survival_Features_Labeled.csv\".format(str_date_today)).iloc[:, 1:]\n",
    "    print(\"Columns Before Processing: \", list(pdf14_labeled.columns))\n",
    "    tpdf01_label = pdf14_labeled.iloc[:, 0]\n",
    "    tpdf02_features = pdf14_labeled.iloc[:, 8:]\n",
    "    pdf14_labeled = pd.concat([tpdf01_label, tpdf02_features], axis=1)\n",
    "    pdf14_labeled = pdf14_labeled.dropna().reset_index(drop=True)\n",
    "    pdf14_labeled = pdf14_labeled.loc[pdf14_labeled[\"posted_speed\"].apply(lambda x: pd.Series([x]).str.isnumeric()).iloc[:,0], :]\n",
    "    pdf14_labeled = pdf14_labeled.reset_index(drop=True)\n",
    "    #\n",
    "    ### Clean column names\n",
    "    pdf01_cols = list(pdf14_labeled.columns)\n",
    "    pdf01_cols = [str(i).lower().replace(\" \",\"_\").replace(r\"/\",\"_\").replace(\",\",\"\").replace(\".\",\"_\").replace(\"-\",\"_\") for i in pdf01_cols]\n",
    "    pdf14_labeled.columns = pdf01_cols\n",
    "    #\n",
    "    print(\"\")\n",
    "    print(\"\\nColumns After Processing: \", list(pdf14_labeled.columns))\n",
    "    #\n",
    "    return pdf14_labeled\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221074ef-2ce4-4fc0-98a2-88ee0c77a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_model(\n",
    "        pdf14_labeled:pd.DataFrame\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #\n",
    "    start_time = time.time()\n",
    "    #\n",
    "    X = pdf14_labeled.iloc[:, 1:].copy()\n",
    "    y = pdf14_labeled.iloc[:, 0].copy()\n",
    "    print(\"Features To Scale and Use: \")\n",
    "    print(list(X.columns))\n",
    "    print(\"\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "    #\n",
    "    ### This section is built to select which features to use in the final model. ###\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    #\n",
    "    ### Tune Hyperparameter\n",
    "    print(\"Use Grid Search to Tune Hyperparameter.\")\n",
    "    model_forest = RandomForestClassifier(random_state = 25)\n",
    "    gscv_rfc = GridSearchCV(model_forest, param_grid={\"n_estimators\":[4,16,256], \"max_depth\":[2,8,16]})\n",
    "    gscv_rfc.fit(X_train_scaled, y_train)\n",
    "    print(\"The Best Params Are: {}\".format(gscv_rfc.best_params_))\n",
    "    #\n",
    "    model_forest = RandomForestClassifier(random_state = 25, \n",
    "        max_depth = gscv_rfc.best_params_[\"max_depth\"], \n",
    "        n_estimators = gscv_rfc.best_params_[\"n_estimators\"])\n",
    "    model_forest.fit(X_train_scaled, y_train)\n",
    "    feature_importances = model_forest.feature_importances_\n",
    "    X_cols = list(X_train.columns)\n",
    "    tpdf01_features = pd.DataFrame({\"feature\":X_cols, \"score\":feature_importances})\n",
    "    tpdf01_features.sort_values(by=[\"score\",\"feature\"], ascending=[False,True], ignore_index=True, inplace=True)\n",
    "    X_cols_sorted = list(tpdf01_features[\"feature\"])\n",
    "    best_score = 0\n",
    "    best_features = 0\n",
    "    print(\"\")\n",
    "    print(\"Find the Best Combination of Features\")\n",
    "    for idx in range(1,len(X_cols_sorted)+1):\n",
    "        print(\"Processing Feature Number {} From {} Features\".format(idx, len(X_cols_sorted)))\n",
    "        i_cols = X_cols_sorted[:idx].copy()\n",
    "        i_X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns).loc[:, i_cols].copy()\n",
    "        i_X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns).loc[:, i_cols].copy()\n",
    "        i_model_forest = RandomForestClassifier(random_state=25)\n",
    "        i_model_forest.fit(i_X_train_scaled, y_train)\n",
    "        i_predict_forest = i_model_forest.predict(i_X_test_scaled)\n",
    "        i_score = accuracy_score(y_test, i_predict_forest)\n",
    "        if i_score > best_score:\n",
    "            best_score = i_score\n",
    "            best_features = i_cols\n",
    "        #\n",
    "    print(\"\")\n",
    "    print(\"The Best Score Is: {:.2f}%\".format(best_score*100))\n",
    "    print(\"The Best Features Are:\")\n",
    "    print(best_features)\n",
    "    print(\"\")\n",
    "    #################################################################################\n",
    "    #\n",
    "    ### This section is to take the learnings from the previous section to train the model. ###\n",
    "    X_train_selected = X_train.loc[:, best_features].copy()\n",
    "    X_test_selected = X_test.loc[:, best_features].copy()\n",
    "    #\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_selected)\n",
    "    X_train_scaled_selected = scaler.transform(X_train_selected)\n",
    "    X_test_scaled_selected = scaler.transform(X_test_selected)\n",
    "    #\n",
    "    model_forest = RandomForestClassifier(random_state = 25, \n",
    "        max_depth = gscv_rfc.best_params_[\"max_depth\"], \n",
    "        n_estimators = gscv_rfc.best_params_[\"n_estimators\"])\n",
    "    model_forest.fit(X_train_scaled_selected, y_train)\n",
    "    predict_forest = model_forest.predict(X_test_scaled_selected)\n",
    "    print(\"Final Accuracy Score Is: {:.2f}%\".format(accuracy_score(y_test, predict_forest)*100))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predict_forest))\n",
    "    print(\"\")\n",
    "    #\n",
    "    ### Create DataFrame to Show Original Values\n",
    "    X_selected_scaled = StandardScaler().fit_transform(X.loc[:, best_features])\n",
    "    X_selected_scaled = pd.DataFrame(X_selected_scaled, columns=best_features)\n",
    "    y_predict = model_forest.predict(X_selected_scaled)\n",
    "    pdf15_classification_model = X.copy().loc[:, best_features]\n",
    "    pdf15_classification_model[\"flag_restaurant_one_year\"] = y\n",
    "    pdf15_classification_model[\"flag_restaurant_one_year_predict\"] = y_predict\n",
    "    print(\"Accuracy Score On Original Dataset Is: {:.2f}%\".format(accuracy_score(y,y_predict)*100))\n",
    "    print(\"\")\n",
    "    #\n",
    "    with open(\"./../models/scaler.pkl\",\"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"Done with exporting scaler as a pickle file.\")\n",
    "    with open(\"./../models/model_forest.pkl\",\"wb\") as f:\n",
    "        pickle.dump(model_forest, f)\n",
    "    print(\"Done with exporting model pickle file.\")\n",
    "    print(\"\")\n",
    "    ###########################################################################################\n",
    "    #\n",
    "    print(\"Done in {:.2f} s\".format(time.time()-start_time))\n",
    "    #\n",
    "    return pdf15_classification_model\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c307d-4c51-476a-8a3b-07f52a5510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_restaurant_survival_prediction(\n",
    "        ):\n",
    "    \"\"\"\n",
    "    For more information, visit:\n",
    "    \"\"\"\n",
    "    #\n",
    "    start_time = time.time()\n",
    "    #\n",
    "    options = {}\n",
    "    options[\"date_today\"] = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "    pdf13_all_features = get_all_features(spark, options)\n",
    "    pdf14_labeled = read_data_processed()\n",
    "    pdf15_classification_model = get_classification_model(pdf14_labeled)\n",
    "    #\n",
    "    pdf15_classification_model.to_csv(\"./../data/processed/survival_predicted.csv\")\n",
    "    #\n",
    "    print(\"run_restaurant_survival_prediction | Done in {:.2f} s\".format(time.time()-start_time))\n",
    "    #\n",
    "    return pdf15_classification_model\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24612a9-0bd6-4bc0-b53b-9aad6c5916cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf15_classification_model = run_restaurant_survival_prediction()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2b51c-2743-4054-a051-792fda6037cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
